{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7405\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_decomposition</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answers</th>\n",
       "      <th>predicted_triples</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>score</th>\n",
       "      <th>n_hops</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1</th>\n",
       "      <th>fuzzy_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a8b57f25542995d1e6f1371</td>\n",
       "      <td>Were Scott Derrickson and Ed Wood of the same ...</td>\n",
       "      <td>[{'answer': '', 'id': 0, 'paragraph_support_id...</td>\n",
       "      <td># Scott Derrickson\\nScott Derrickson (born Jul...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[yes]</td>\n",
       "      <td>Scott Derrickson, nationality, American\\nEd Wo...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a8c7595554299585d9e36b6</td>\n",
       "      <td>What government position was held by the woman...</td>\n",
       "      <td>[{'answer': '', 'id': 0, 'paragraph_support_id...</td>\n",
       "      <td># Shirley Temple\\nShirley Temple Black (April ...</td>\n",
       "      <td>Chief of Protocol</td>\n",
       "      <td>[Chief of Protocol]</td>\n",
       "      <td>Shirley Temple, portrayed, Corliss Archer\\nShi...</td>\n",
       "      <td>United States ambassador and Chief of Protocol...</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a85ea095542994775f606a8</td>\n",
       "      <td>What science fantasy young adult series, told ...</td>\n",
       "      <td>[{'answer': '', 'id': 0, 'paragraph_support_id...</td>\n",
       "      <td># The Hork-Bajir Chronicles\\nThe Hork-Bajir Ch...</td>\n",
       "      <td>Animorphs</td>\n",
       "      <td>[Animorphs]</td>\n",
       "      <td>Animorphs, written by, K. A. Applegate\\nThe Ho...</td>\n",
       "      <td>Animorphs</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adbf0a255429947ff17385a</td>\n",
       "      <td>Are the Laleli Mosque and Esma Sultan Mansion ...</td>\n",
       "      <td>[{'answer': '', 'id': 0, 'paragraph_support_id...</td>\n",
       "      <td># Laleli Mosque\\nThe Laleli Mosque (Turkish: \"...</td>\n",
       "      <td>no</td>\n",
       "      <td>[no]</td>\n",
       "      <td>Laleli Mosque, located_in, Fatih\\nEsma Sultan ...</td>\n",
       "      <td>No</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a8e3ea95542995a26add48d</td>\n",
       "      <td>The director of the romantic comedy \"Big Stone...</td>\n",
       "      <td>[{'answer': '', 'id': 0, 'paragraph_support_id...</td>\n",
       "      <td># Adriana Trigiani\\nAdriana Trigiani is an Ita...</td>\n",
       "      <td>Greenwich Village, New York City</td>\n",
       "      <td>[Greenwich Village, New York City]</td>\n",
       "      <td>Adriana Trigiani, based in, Greenwich Village\\...</td>\n",
       "      <td>New York City</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5a8b57f25542995d1e6f1371   \n",
       "1  5a8c7595554299585d9e36b6   \n",
       "2  5a85ea095542994775f606a8   \n",
       "3  5adbf0a255429947ff17385a   \n",
       "4  5a8e3ea95542995a26add48d   \n",
       "\n",
       "                                            question  \\\n",
       "0  Were Scott Derrickson and Ed Wood of the same ...   \n",
       "1  What government position was held by the woman...   \n",
       "2  What science fantasy young adult series, told ...   \n",
       "3  Are the Laleli Mosque and Esma Sultan Mansion ...   \n",
       "4  The director of the romantic comedy \"Big Stone...   \n",
       "\n",
       "                              question_decomposition  \\\n",
       "0  [{'answer': '', 'id': 0, 'paragraph_support_id...   \n",
       "1  [{'answer': '', 'id': 0, 'paragraph_support_id...   \n",
       "2  [{'answer': '', 'id': 0, 'paragraph_support_id...   \n",
       "3  [{'answer': '', 'id': 0, 'paragraph_support_id...   \n",
       "4  [{'answer': '', 'id': 0, 'paragraph_support_id...   \n",
       "\n",
       "                                             context  \\\n",
       "0  # Scott Derrickson\\nScott Derrickson (born Jul...   \n",
       "1  # Shirley Temple\\nShirley Temple Black (April ...   \n",
       "2  # The Hork-Bajir Chronicles\\nThe Hork-Bajir Ch...   \n",
       "3  # Laleli Mosque\\nThe Laleli Mosque (Turkish: \"...   \n",
       "4  # Adriana Trigiani\\nAdriana Trigiani is an Ita...   \n",
       "\n",
       "                             answer                             answers  \\\n",
       "0                               yes                               [yes]   \n",
       "1                 Chief of Protocol                 [Chief of Protocol]   \n",
       "2                         Animorphs                         [Animorphs]   \n",
       "3                                no                                [no]   \n",
       "4  Greenwich Village, New York City  [Greenwich Village, New York City]   \n",
       "\n",
       "                                   predicted_triples  \\\n",
       "0  Scott Derrickson, nationality, American\\nEd Wo...   \n",
       "1  Shirley Temple, portrayed, Corliss Archer\\nShi...   \n",
       "2  Animorphs, written by, K. A. Applegate\\nThe Ho...   \n",
       "3  Laleli Mosque, located_in, Fatih\\nEsma Sultan ...   \n",
       "4  Adriana Trigiani, based in, Greenwich Village\\...   \n",
       "\n",
       "                                    predicted_answer     score  n_hops  \\\n",
       "0                                                Yes  1.000000       2   \n",
       "1  United States ambassador and Chief of Protocol...  0.461538       3   \n",
       "2                                          Animorphs  1.000000       5   \n",
       "3                                                 No  1.000000       2   \n",
       "4                                      New York City  0.750000       2   \n",
       "\n",
       "   exact_match        f1  fuzzy_match  \n",
       "0            1  1.000000            0  \n",
       "1            0  0.461538            0  \n",
       "2            1  1.000000            1  \n",
       "3            1  1.000000            0  \n",
       "4            0  0.750000            0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('hotpotqa-cte-inspect.jsonl', lines=True)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       |      n_hops |   exact_match |          f1 |\n",
      "|:------|------------:|--------------:|------------:|\n",
      "| count | 7405        |   7405        | 7405        |\n",
      "| mean  |    2.43147  |      0.664281 |    0.816989 |\n",
      "| std   |    0.711531 |      0.472273 |    0.315591 |\n",
      "| min   |    2        |      0        |    0        |\n",
      "| 25%   |    2        |      0        |    0.666667 |\n",
      "| 50%   |    2        |      1        |    1        |\n",
      "| 75%   |    3        |      1        |    1        |\n",
      "| max   |    8        |      1        |    1        |\n"
     ]
    }
   ],
   "source": [
    "print(df[['n_hops', 'exact_match', 'f1']].describe().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766 6639 7405\n",
      "0.897\n"
     ]
    }
   ],
   "source": [
    "success_mask = df['f1'] > 0.3\n",
    "fail_df = df[~success_mask]\n",
    "success_df = df[success_mask]\n",
    "print(len(fail_df), len(success_df), len(df))\n",
    "print(\"{:.3f}\".format(len(success_df)/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">exact_match</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_hops</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666733</td>\n",
       "      <td>4990</td>\n",
       "      <td>0.815248</td>\n",
       "      <td>4990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.643179</td>\n",
       "      <td>1774</td>\n",
       "      <td>0.810292</td>\n",
       "      <td>1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.698324</td>\n",
       "      <td>537</td>\n",
       "      <td>0.844756</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.775000</td>\n",
       "      <td>80</td>\n",
       "      <td>0.885774</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>14</td>\n",
       "      <td>0.840476</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>9</td>\n",
       "      <td>0.776720</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       exact_match              f1      \n",
       "              mean count      mean count\n",
       "n_hops                                  \n",
       "2         0.666733  4990  0.815248  4990\n",
       "3         0.643179  1774  0.810292  1774\n",
       "4         0.698324   537  0.844756   537\n",
       "5         0.775000    80  0.885774    80\n",
       "6         0.642857    14  0.840476    14\n",
       "7         0.444444     9  0.776720     9\n",
       "8         1.000000     1  1.000000     1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('n_hops')[['exact_match', 'f1']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential_jitter\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "DEFAULT_MODEL = \"llama-3-70b-tgi\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "\n",
    "INSPECT_PROMPT = \"\"\"\n",
    "Analyze the following failed Q&A data point and categorize the failure(s) into one or more of these types:\n",
    "1. Granularity mismatch.\n",
    "2. Semantic equivalence mismatch.\n",
    "3. Temporal or hierarchical reasoning error.\n",
    "4. Incorrect entity or relationship focus.\n",
    "5. Formatting inconsistency.\n",
    "6. Logical inconsistency in reasoning.\n",
    "7. Incorrect synthesis of multiple hops.\n",
    "8. Reference answer ambiguity or error.\n",
    "\n",
    "Data:\n",
    "- Context: \n",
    "{context}\n",
    "\n",
    "- Question: {question}\n",
    "\n",
    "- Reference Answer(s): {answers}\n",
    "\n",
    "- Predicted triplets: \n",
    "{predicted_triples}\n",
    "\n",
    "- Predicted Answer: {predicted_answer}\n",
    "\n",
    "- Evaluation Metrics: EM = {exact_match:.3f}, F1 = {f1:.3f}\n",
    "\n",
    "Output:\n",
    "- Failure Categories:\n",
    "- Explanation:\n",
    "\"\"\".strip()\n",
    "\n",
    "SUMMARIZE_PROMPT = \"\"\"\n",
    "Summarize the failure analysis results of Q&A data points. Include:\n",
    "1. The frequency of each failure mode.\n",
    "2. Common examples of failure modes.\n",
    "3. Rare examples of failure modes.\n",
    "4. Observations about systemic issues in the predictions.\n",
    "\n",
    "Analysis data:\n",
    "{categorized_failure_data}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(row):\n",
    "    return INSPECT_PROMPT.format(\n",
    "        question=row[\"question\"],\n",
    "        answers=row[\"answers\"],\n",
    "        predicted_triples=row[\"predicted_triples\"],\n",
    "        predicted_answer=row[\"predicted_answer\"],\n",
    "        context=row[\"context\"],\n",
    "        exact_match=row[\"exact_match\"],\n",
    "        f1=row[\"f1\"],\n",
    "    )\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential_jitter(max=10))\n",
    "async def chat_completion(\n",
    "    messages,\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "async def analyze_data_point(row):\n",
    "    prompt = format_prompt(row)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant for analyzing Q&A model failures.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    return await chat_completion(messages)\n",
    "\n",
    "\n",
    "async def process_batch(batch):\n",
    "    tasks = [analyze_data_point(row) for _, row in batch.iterrows()]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def save_jsonl(filename, data):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_completed_batches(out_dir):\n",
    "    completed_files = list(out_dir.glob(\"batch_*_results.jsonl\"))\n",
    "    completed_batches = [int(file.stem.split(\"_\")[1]) for file in completed_files]\n",
    "    return set(completed_batches)\n",
    "\n",
    "\n",
    "async def process_batches(dataframe, out_dir):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    completed_batches = get_completed_batches(out_dir)\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(dataframe), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_index = i // BATCH_SIZE\n",
    "        if batch_index in completed_batches:\n",
    "            continue\n",
    "\n",
    "        batch = dataframe.iloc[i : i + BATCH_SIZE]\n",
    "        batch_results = await process_batch(batch)\n",
    "        batch_with_ids = [\n",
    "            {\"id\": row[\"id\"], \"analysis\": analysis}\n",
    "            for row, analysis in zip(batch.to_dict(\"records\"), batch_results)\n",
    "        ]\n",
    "        results.extend(batch_with_ids)\n",
    "        save_jsonl(out_dir / f\"batch_{batch_index}_results.jsonl\", batch_with_ids)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def summarize_results_in_chunks(results, chunk_size=100, model=DEFAULT_MODEL):\n",
    "    chunk_summaries = []\n",
    "\n",
    "    # Divide results into smaller chunks\n",
    "    for i in tqdm(range(0, len(results), chunk_size), desc=\"Summarizing chunks\"):\n",
    "        chunk = results[i : i + chunk_size]\n",
    "        summary_prompt = SUMMARIZE_PROMPT.format(categorized_failure_data=chunk)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant for analyzing Q&A model failures.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": summary_prompt},\n",
    "        ]\n",
    "        chunk_summary = await chat_completion(messages, model=model)\n",
    "        chunk_summaries.append(chunk_summary)\n",
    "\n",
    "    # Combine chunk summaries into a final summary\n",
    "    final_prompt = \"\"\"\n",
    "    Combine the following summaries into a single cohesive summary:\n",
    "    {chunk_summaries}\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant for summarizing Q&A analysis summaries.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_prompt.format(\n",
    "                chunk_summaries=\"\\n\\n\".join(chunk_summaries)\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    final_summary = await chat_completion(messages, model=model)\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"../tmp/hotpotqa-auto-inspect-cte3/\")\n",
    "ANALYSIS_DIR = OUT_DIR / \"analysis_results\"\n",
    "SUMMARY_FILE = OUT_DIR / \"summary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 77/77 [1:12:01<00:00, 56.12s/it]\n"
     ]
    }
   ],
   "source": [
    "results = await process_batches(fail_df, ANALYSIS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for filepath in ANALYSIS_DIR.glob(\"*.jsonl\"):\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing chunks: 100%|██████████| 26/26 [04:05<00:00,  9.43s/it]\n"
     ]
    }
   ],
   "source": [
    "summary = await summarize_results_in_chunks(results, chunk_size=30, model=\"gpt-4o-mini\")\n",
    "with open(SUMMARY_FILE, \"w\") as f:\n",
    "    f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
