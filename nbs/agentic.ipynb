{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "import typer\n",
    "from bellem.musique.eval import (\n",
    "    aggregate_scores,\n",
    "    compute_scores,\n",
    "    compute_scores_dataframe,\n",
    ")\n",
    "from bellem.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt.ensemble import Ensemble\n",
    "from rich.console import Console\n",
    "\n",
    "print = Console(stderr=True).print\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x70b8b367b860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "weave.init(project_name=\"mhqa-dspy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "# mlflow.set_experiment(\"mhqa-dspy\")\n",
    "# mlflow.dspy.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_lm(model, temperature):\n",
    "    lm = dspy.LM(\n",
    "        \"openai/\" + model,\n",
    "        temperature=temperature,\n",
    "        cache=False,\n",
    "        api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5Ranker model unicamp-dl/mt5-base-mmarco-v2 (this message can be suppressed by setting verbose=0)\n",
      "You don't have the necessary dependencies installed to use T5Ranker.\n",
      "Please install the necessary dependencies for T5Ranker by running `pip install \"rerankers[transformers]\"` or `pip install \"rerankers[all]\" to install the dependencies for all reranker types.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Ranker is not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m ranker \u001b[39m=\u001b[39m Reranker(model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mt5\u001b[39m\u001b[39m\"\u001b[39m, model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39municamp-dl/mt5-base-mmarco-v2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m ranker \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRanker is not initialized\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mretrieve\u001b[39m(docs: \u001b[39mlist\u001b[39m[\u001b[39mdict\u001b[39m], query: \u001b[39mstr\u001b[39m, top_k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mdict\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Reranker retriever implementation.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m        List of documents sorted by relevance score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Ranker is not initialized"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(model_type=\"t5\", model_name=\"unicamp-dl/mt5-base-mmarco-v2\")\n",
    "if ranker is None:\n",
    "    raise ValueError(\"Ranker is not initialized\")\n",
    "\n",
    "def retrieve(docs: list[dict], query: str, top_k: int = 3) -> list[dict]:\n",
    "    \"\"\"Reranker retriever implementation.\n",
    "\n",
    "    Args:\n",
    "        docs: List of documents to search in. Each document should be a dict with 'idx' and 'text' fields.\n",
    "        query: Query string to search for\n",
    "        top_k: Number of documents to retrieve (default: 3)\n",
    "\n",
    "    Returns:\n",
    "        List of documents sorted by relevance score\n",
    "    \"\"\"\n",
    "    # Extract text and ids from docs\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    ranking = ranker.rank(query=query, docs=texts, doc_ids=list(range(len(texts))))\n",
    "    return [docs[result.doc_id] for result in ranking.results[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThoughts and Planned Improvements for dspy.ReAct.\\n\\nTOPIC 01: How Trajectories are Formatted, or rather when they are formatted.\\n\\nRight now, both sub-modules are invoked with a `trajectory` argument, which is a string formatted in `forward`. Though\\nthe formatter uses a general adapter.format_fields, the tracing of DSPy only sees the string, not the formatting logic.\\n\\nWhat this means is that, in demonstrations, even if the user adjusts the adapter for a fixed program, the demos\\' format\\nwill not update accordingly, but the inference-time trajectories will.\\n\\nOne way to fix this is to support `format=fn` in the dspy.InputField() for \"trajectory\" in the signatures. But this\\nmeans that care must be taken that the adapter is accessed at `forward` runtime, not signature definition time.\\n\\nAnother potential fix is to more natively support a \"variadic\" input field, where the input is a list of dictionaries,\\nor a big dictionary, and have each adatper format it accordingly.\\n\\nTrajectories also affect meta-programming modules that view the trace later. It\\'s inefficient O(n^2) to view the\\ntrace of every module repeating the prefix.\\n\\n\\nTOPIC 02: Handling default arguments in the Tool class.\\n\\n\\nTOPIC 03: Simplifying ReAct\\'s __init__ by moving modular logic to the Tool class.\\n    * Handling descriptions and casting.\\n    * Handling exceptions and error messages.\\n    * More cleanly defining the \"finish\" tool, perhaps as a runtime-defined function?\\n\\n\\nTOPIC 04: Default behavior when the trajectory gets too long.\\n\\n\\nTOPIC 05: Adding more structure around how the instruction is formatted.\\n    * Concretely, it\\'s now a string, so an optimizer can and does rewrite it freely.\\n    * An alternative would be to add more structure, such that a certain template is fixed but values are variable?\\n\\n\\nTOPIC 06: Idiomatically allowing tools that maintain state across iterations, but not across different `forward` calls.\\n    * So the tool would be newly initialized at the start of each `forward` call, but maintain state across iterations.\\n    * This is pretty useful for allowing the agent to keep notes or count certain things, etc.\\n\\nTOPIC 07: Make max_iters a bit more expressive.\\n    * Allow passing `max_iters` in forward to overwrite the default.\\n    * Get rid of `last_iteration: bool` in the format function. It\\'s not necessary now.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Literal, get_origin, get_type_hints\n",
    "\n",
    "import dspy\n",
    "from dspy.primitives.program import Module\n",
    "from dspy.signatures.signature import ensure_signature\n",
    "from dspy.utils.callback import with_callbacks\n",
    "from pydantic import BaseModel, TypeAdapter\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunContext:\n",
    "    input: dict[str, Any]\n",
    "\n",
    "\n",
    "class Tool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        func: Callable,\n",
    "        name: str = None,\n",
    "        desc: str = None,\n",
    "        args: dict[str, Any] | None = None,\n",
    "    ):\n",
    "        annotations_func = func if inspect.isfunction(func) or inspect.ismethod(func) else func.__call__\n",
    "\n",
    "        self.func = func\n",
    "        self.name = name or getattr(func, \"__name__\", type(func).__name__)\n",
    "        self.desc = desc or getattr(func, \"__doc__\", None) or getattr(annotations_func, \"__doc__\", \"\")\n",
    "        self.args = {}\n",
    "        self.arg_types = {}\n",
    "\n",
    "        # If an explicit args dict is passed, use that; otherwise, extract from the function.\n",
    "        if args is not None:\n",
    "            hints = args\n",
    "        else:\n",
    "            # Use inspect.signature to get all parameter names\n",
    "            sig = inspect.signature(annotations_func)\n",
    "            # Get available type hints\n",
    "            available_hints = get_type_hints(annotations_func)\n",
    "            # Build a dictionary of parameter name -> type (defaulting to Any when missing)\n",
    "            hints = {param_name: available_hints.get(param_name, Any) for param_name in sig.parameters.keys()}\n",
    "\n",
    "        # Process each argument's type to generate its JSON schema.\n",
    "        for k, v in hints.items():\n",
    "            self.arg_types[k] = v\n",
    "            if k == \"return\":\n",
    "                continue\n",
    "            if k == \"run_context\":\n",
    "                continue\n",
    "            # Check if the type (or its origin) is a subclass of Pydantic's BaseModel\n",
    "            origin = get_origin(v) or v\n",
    "            if isinstance(origin, type) and issubclass(origin, BaseModel):\n",
    "                self.args[k] = v.model_json_schema()\n",
    "            else:\n",
    "                self.args[k] = TypeAdapter(v).json_schema() or \"Any\"\n",
    "\n",
    "    @with_callbacks\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "\n",
    "class ReAct(Module):\n",
    "    def __init__(self, signature, tools: list[Callable], max_iters=5):\n",
    "        \"\"\"\n",
    "        `tools` is either a list of functions, callable classes, or `dspy.Tool` instances.\n",
    "        \"\"\"\n",
    "\n",
    "        self.signature = signature = ensure_signature(signature)\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "        tools = [t if isinstance(t, Tool) or hasattr(t, \"input_variable\") else Tool(t) for t in tools]\n",
    "        tools = {tool.name: tool for tool in tools}\n",
    "\n",
    "        inputs = \", \".join([f\"`{k}`\" for k in signature.input_fields.keys()])\n",
    "        outputs = \", \".join([f\"`{k}`\" for k in signature.output_fields.keys()])\n",
    "        instr = [f\"{signature.instructions}\\n\"] if signature.instructions else []\n",
    "\n",
    "        instr.extend(\n",
    "            [\n",
    "                f\"You will be given {inputs} and your goal is to finish with {outputs}.\\n\",\n",
    "                \"To do this, you will interleave Thought, Tool Name, and Tool Args, and receive a resulting Observation.\\n\",\n",
    "                \"Thought can reason about the current situation, and Tool Name can be the following types:\\n\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        finish_desc = (\n",
    "            f\"Signals that the final outputs, i.e. {outputs}, are now available and marks the task as complete.\"\n",
    "        )\n",
    "        finish_args = {}  # k: v.annotation for k, v in signature.output_fields.items()}\n",
    "        tools[\"finish\"] = Tool(\n",
    "            func=lambda **kwargs: \"Completed.\",\n",
    "            name=\"finish\",\n",
    "            desc=finish_desc,\n",
    "            args=finish_args,\n",
    "        )\n",
    "\n",
    "        for idx, tool in enumerate(tools.values()):\n",
    "            args = tool.args if hasattr(tool, \"args\") else str({tool.input_variable: str})\n",
    "            desc = (f\", whose description is <desc>{tool.desc}</desc>.\" if tool.desc else \".\").replace(\"\\n\", \"  \")\n",
    "            desc += f\" It takes arguments {args} in JSON format.\"\n",
    "            instr.append(f\"({idx + 1}) {tool.name}{desc}\")\n",
    "\n",
    "        react_signature = (\n",
    "            dspy.Signature({**signature.input_fields}, \"\\n\".join(instr))\n",
    "            .append(\"trajectory\", dspy.InputField(), type_=str)\n",
    "            .append(\"next_thought\", dspy.OutputField(), type_=str)\n",
    "            .append(\"next_tool_name\", dspy.OutputField(), type_=Literal[tuple(tools.keys())])\n",
    "            .append(\"next_tool_args\", dspy.OutputField(), type_=dict[str, Any])\n",
    "        )\n",
    "\n",
    "        fallback_signature = dspy.Signature(\n",
    "            {**signature.input_fields, **signature.output_fields},\n",
    "            signature.instructions,\n",
    "        ).append(\"trajectory\", dspy.InputField(), type_=str)\n",
    "\n",
    "        self.tools = tools\n",
    "        self.react = dspy.Predict(react_signature)\n",
    "        self.extract = dspy.ChainOfThought(fallback_signature)\n",
    "\n",
    "    def forward(self, **input_args):\n",
    "        def format(trajectory: dict[str, Any], last_iteration: bool):\n",
    "            adapter = dspy.settings.adapter or dspy.ChatAdapter()\n",
    "            trajectory_signature = dspy.Signature(f\"{', '.join(trajectory.keys())} -> x\")\n",
    "            return adapter.format_fields(trajectory_signature, trajectory, role=\"user\")\n",
    "\n",
    "        trajectory = {}\n",
    "        for idx in range(self.max_iters):\n",
    "            pred = self.react(\n",
    "                **input_args,\n",
    "                trajectory=format(trajectory, last_iteration=(idx == self.max_iters - 1)),\n",
    "            )\n",
    "\n",
    "            trajectory[f\"thought_{idx}\"] = pred.next_thought\n",
    "            trajectory[f\"tool_name_{idx}\"] = pred.next_tool_name\n",
    "            trajectory[f\"tool_args_{idx}\"] = pred.next_tool_args\n",
    "\n",
    "            try:\n",
    "                parsed_tool_args = {}\n",
    "                tool = self.tools[pred.next_tool_name]\n",
    "                for k, v in pred.next_tool_args.items():\n",
    "                    if hasattr(tool, \"arg_types\") and k in tool.arg_types:\n",
    "                        arg_type = tool.arg_types[k]\n",
    "                        if isinstance((origin := get_origin(arg_type) or arg_type), type) and issubclass(\n",
    "                            origin, BaseModel\n",
    "                        ):\n",
    "                            parsed_tool_args[k] = arg_type.model_validate(v)\n",
    "                            continue\n",
    "                    parsed_tool_args[k] = v\n",
    "                trajectory[f\"observation_{idx}\"] = self.tools[pred.next_tool_name](\n",
    "                    **parsed_tool_args,\n",
    "                    run_context=RunContext(input=input_args),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                trajectory[f\"observation_{idx}\"] = f\"Failed to execute: {e}\"\n",
    "\n",
    "            if pred.next_tool_name == \"finish\":\n",
    "                break\n",
    "\n",
    "        extract = self.extract(**input_args, trajectory=format(trajectory, last_iteration=False))\n",
    "        return dspy.Prediction(trajectory=trajectory, **extract)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Thoughts and Planned Improvements for dspy.ReAct.\n",
    "\n",
    "TOPIC 01: How Trajectories are Formatted, or rather when they are formatted.\n",
    "\n",
    "Right now, both sub-modules are invoked with a `trajectory` argument, which is a string formatted in `forward`. Though\n",
    "the formatter uses a general adapter.format_fields, the tracing of DSPy only sees the string, not the formatting logic.\n",
    "\n",
    "What this means is that, in demonstrations, even if the user adjusts the adapter for a fixed program, the demos' format\n",
    "will not update accordingly, but the inference-time trajectories will.\n",
    "\n",
    "One way to fix this is to support `format=fn` in the dspy.InputField() for \"trajectory\" in the signatures. But this\n",
    "means that care must be taken that the adapter is accessed at `forward` runtime, not signature definition time.\n",
    "\n",
    "Another potential fix is to more natively support a \"variadic\" input field, where the input is a list of dictionaries,\n",
    "or a big dictionary, and have each adatper format it accordingly.\n",
    "\n",
    "Trajectories also affect meta-programming modules that view the trace later. It's inefficient O(n^2) to view the\n",
    "trace of every module repeating the prefix.\n",
    "\n",
    "\n",
    "TOPIC 02: Handling default arguments in the Tool class.\n",
    "\n",
    "\n",
    "TOPIC 03: Simplifying ReAct's __init__ by moving modular logic to the Tool class.\n",
    "    * Handling descriptions and casting.\n",
    "    * Handling exceptions and error messages.\n",
    "    * More cleanly defining the \"finish\" tool, perhaps as a runtime-defined function?\n",
    "\n",
    "\n",
    "TOPIC 04: Default behavior when the trajectory gets too long.\n",
    "\n",
    "\n",
    "TOPIC 05: Adding more structure around how the instruction is formatted.\n",
    "    * Concretely, it's now a string, so an optimizer can and does rewrite it freely.\n",
    "    * An alternative would be to add more structure, such that a certain template is fixed but values are variable?\n",
    "\n",
    "\n",
    "TOPIC 06: Idiomatically allowing tools that maintain state across iterations, but not across different `forward` calls.\n",
    "    * So the tool would be newly initialized at the start of each `forward` call, but maintain state across iterations.\n",
    "    * This is pretty useful for allowing the agent to keep notes or count certain things, etc.\n",
    "\n",
    "TOPIC 07: Make max_iters a bit more expressive.\n",
    "    * Allow passing `max_iters` in forward to overwrite the default.\n",
    "    * Get rid of `last_iteration: bool` in the format function. It's not necessary now.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paragraph(paragraph):\n",
    "    text = paragraph[\"paragraph_text\"]\n",
    "    title = paragraph[\"title\"]\n",
    "    return f\"# {title}\\n{text}\"\n",
    "\n",
    "\n",
    "def make_example(record):\n",
    "    docs = [{\"text\": format_paragraph(p), \"idx\": p[\"idx\"]} for p in record[\"paragraphs\"]]\n",
    "    return dspy.Example(\n",
    "        id=record[\"id\"],\n",
    "        question=record[\"question\"],\n",
    "        docs=docs,\n",
    "        question_decomposition=record[\"question_decomposition\"],\n",
    "        answers=[record[\"answer\"], *record[\"answer_aliases\"]],\n",
    "    ).with_inputs(\"question\", \"docs\")\n",
    "\n",
    "\n",
    "def search(query: str, run_context: RunContext) -> list[str]:\n",
    "    \"\"\"Find relevant documents for the query.\"\"\"\n",
    "    retrieved_docs = retrieve(run_context.input[\"docs\"], query, 3)\n",
    "    return [x[\"text\"] for x in retrieved_docs]\n",
    "\n",
    "\n",
    "def make_program():\n",
    "    return ReAct(\"question -> answer\", tools=[search])\n",
    "\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    scores = compute_scores(pred.answer, example.answers)\n",
    "    return scores[\"f1\"]\n",
    "\n",
    "\n",
    "def dynamic_import(module, name):\n",
    "    import importlib\n",
    "\n",
    "    return getattr(importlib.import_module(module), name)\n",
    "\n",
    "\n",
    "def make_optimizer(optimizer_config: dict):\n",
    "    cls = dynamic_import(\"dspy.teleprompt\", optimizer_config[\"class\"])\n",
    "    kwargs = deepcopy(optimizer_config[\"params\"])\n",
    "    if optimizer_config[\"with_metric\"]:\n",
    "        kwargs[\"metric\"] = evaluate_answer\n",
    "    return cls(**kwargs)\n",
    "\n",
    "\n",
    "def preprocess_result(result):\n",
    "    example, pred, score = result\n",
    "    predictions = {f\"predicted_{k}\": v for k, v in dict(pred).items()}\n",
    "    return {**dict(example), **predictions, \"score\": float(score)}\n",
    "\n",
    "\n",
    "def make_results_dataframe(results):\n",
    "    dataf = pd.json_normalize([preprocess_result(result) for result in results])\n",
    "    dataf[\"n_hops\"] = dataf[\"question_decomposition\"].apply(len)\n",
    "    dataf[\"predicted_answer\"] = dataf[\"predicted_answer\"].fillna(\"No Answer\")\n",
    "    return compute_scores_dataframe(dataf)\n",
    "\n",
    "\n",
    "def train_main(\n",
    "    dataset_path: str = typer.Option(..., help=\"Path to the dataset\"),\n",
    "    dataset_name: str = typer.Option(..., help=\"Name of the dataset\"),\n",
    "    dataset_split: str = typer.Option(..., help=\"Dataset split to use (e.g., 'train', 'validation')\"),\n",
    "    model: str = typer.Option(..., help=\"Name of the model to use\"),\n",
    "    temperature: float = typer.Option(..., help=\"Temperature parameter for the model\"),\n",
    "    load_from: str = typer.Option(default=\"UNSET\", help=\"Path to a saved model to load\"),\n",
    "    optimizer_path: Path = typer.Option(..., help=\"Path to the optimizer config\"),\n",
    "    ensemble: str = typer.Option(\"no\", help=\"Whether to use an ensemble of models\"),\n",
    "    out: Path = typer.Option(..., help=\"Output file for trained program\"),\n",
    "):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up LLM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [make_example(record) for record in ds]\n",
    "    print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "    # Create the program\n",
    "    program = make_program()\n",
    "    if load_from and load_from != \"UNSET\":\n",
    "        print(f\"Loading model from {load_from}\")\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Train the program\n",
    "    with open(optimizer_path) as f:\n",
    "        optimizer_config = json.load(f)\n",
    "\n",
    "    if optimizer_config:\n",
    "        optimizer = make_optimizer(optimizer_config)\n",
    "        compile_params = optimizer_config.get(\"compile_params\", {})\n",
    "        trained_program = optimizer.compile(program, trainset=examples, **compile_params)\n",
    "    else:\n",
    "        trained_program = program\n",
    "\n",
    "    if ensemble == \"yes\":\n",
    "        ensemble_optimizer = Ensemble(reduce_fn=dspy.majority)\n",
    "        candidate_programs = [x[-1] for x in trained_program.candidate_programs]\n",
    "        trained_program = ensemble_optimizer.compile(candidate_programs)\n",
    "\n",
    "    # Save the trained program\n",
    "    trained_program.save(out)\n",
    "\n",
    "    return trained_program\n",
    "\n",
    "def evaluate_main(\n",
    "    dataset_path: str = typer.Option(..., help=\"Path to the dataset\"),\n",
    "    dataset_name: str = typer.Option(..., help=\"Name of the dataset\"),\n",
    "    dataset_split: str = typer.Option(..., help=\"Dataset split to use (e.g., 'train', 'validation')\"),\n",
    "    model: str = typer.Option(..., help=\"Name of the model to use\"),\n",
    "    temperature: float = typer.Option(..., help=\"Temperature parameter for the model\"),\n",
    "    load_from: str = typer.Option(default=\"UNSET\", help=\"Path to a saved model to load\"),\n",
    "    out: Path = typer.Option(..., help=\"Output directory for generated results\"),\n",
    "):\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up LLM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [make_example(record) for record in ds]\n",
    "    print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "    # Create the program\n",
    "    program = make_program()\n",
    "    if load_from and load_from != \"UNSET\":\n",
    "        print(f\"Loading model from {load_from}\")\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Evaluate the program\n",
    "    evaluate_program = Evaluate(\n",
    "        metric=evaluate_answer,\n",
    "        devset=examples,\n",
    "        num_threads=4,\n",
    "        display_progress=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    _, results = evaluate_program(program)\n",
    "\n",
    "    # Save the results\n",
    "    result_df = make_results_dataframe(results)\n",
    "    result_df.to_json(out / \"results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # Save the scores\n",
    "    scores = aggregate_scores(result_df)\n",
    "    for n_hops in result_df[\"n_hops\"].unique():\n",
    "        scores[f\"{n_hops}hops\"] = aggregate_scores(result_df[result_df[\"n_hops\"] == n_hops])\n",
    "\n",
    "    with open(out / \"scores.json\", \"w\") as f:\n",
    "        json.dump(scores, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='llama-3.3-70b-tgi'\n",
    "# model='meta-llama/Llama-3.3-70B-Instruct-Turbo'\n",
    "# model='llama3.1:8b-instruct-q8_0'\n",
    "# model='llama-3.1-8b-instant'\n",
    "# model='gemini-2.0-flash-lite-preview-02-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m300\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/11 12:15:29 WARNING dspy.utils.parallelizer: Received SIGINT. Cancelling execution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c363-73a0-9157-b6c9b81fc0f7\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c364-7533-b165-a9cc3e1329a0\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c365-78f0-99a0-9ebcf689d5ee\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c363-73a0-9157-b6d20c890afe\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c361-7663-8e83-54b8028e3a28\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c362-7b00-bed1-eb823137426f\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c365-78f0-99a0-9ec04989931d\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c363-73a0-9157-b6ef824027e3\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c360-7f93-b2f5-21dddd9e762b\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c361-7663-8e83-54aee84b5bae\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c366-7ce0-a3d1-5fe152fcbdac\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c366-7ce0-a3d1-5ff6612f2291\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c360-7f93-b2f5-21cd2e68edab\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c362-7b00-bed1-eb93ab4d75fb\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c366-7ce0-a3d1-60080957de3e\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-c367-7e71-83a6-1bf997a66003\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-dcd4-7060-a457-64263a68595f\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-df78-7c31-a153-aa1731ccd524\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-de62-7c31-a070-be75e17d977f\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-dd82-7641-9974-f6082f4f1aa0\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-dfd1-7893-91cc-234594dd948b\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-dfff-7be3-8574-22c5d810b3be\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-e5a6-7f41-9a4d-b8da34ca504b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-e1ca-7010-88da-ace98b6defef🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-e2e8-78d2-b48d-c86c465b20aa\n",
      "\n",
      "🍩 https://wandb.ai/bdsaglam/mhqa-dspy/r/call/0194f44a-dfea-7d10-a942-6f9dea83ce94\n"
     ]
    }
   ],
   "source": [
    "out = Path('out')\n",
    "\n",
    "trained_program = train_main(\n",
    "    dataset_path='bdsaglam/musique-mini',\n",
    "    dataset_name='answerable',\n",
    "    dataset_split='train',\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    load_from='UNSET',\n",
    "    optimizer_path='../data/raw/optimizer-configs/bfsrs-medium.json',\n",
    "    out=out,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
