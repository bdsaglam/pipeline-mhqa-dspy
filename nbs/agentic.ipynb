{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baris/repos/pipeline-mhqa-dspy/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default t5 model for language en\n",
      "Default Model: unicamp-dl/InRanker-base\n",
      "Loading T5Ranker model unicamp-dl/InRanker-base (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model unicamp-dl/InRanker-base, this might take a while...\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 true token set to ▁true\n",
      "T5 false token set to ▁false\n",
      "Returning normalised scores...\n",
      "Inputs template set to Query: {query} Document: {text} Relevant:\n"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "reranker = Reranker(\"t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bellem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpandas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtyper\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mbellem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmusique\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meval\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     aggregate_scores,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     compute_scores,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     compute_scores_dataframe,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mbellem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m set_seed\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-mhqa-dspy/nbs/agentic.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdatasets\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m load_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bellem'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "import typer\n",
    "from bellem.musique.eval import (\n",
    "    aggregate_scores,\n",
    "    compute_scores,\n",
    "    compute_scores_dataframe,\n",
    ")\n",
    "from bellem.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt.ensemble import Ensemble\n",
    "from rich.console import Console\n",
    "\n",
    "print = Console(stderr=True).print\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import weave\n",
    "# weave.init(project_name=\"mhqa-dspy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "# mlflow.set_experiment(\"mhqa-dspy\")\n",
    "# mlflow.dspy.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def configure_lm(model, temperature):\n",
    "    lm = dspy.LM(\n",
    "        \"openai/\" + model,\n",
    "        temperature=temperature,\n",
    "        cache=False,\n",
    "        api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from mhqa.react import ReAct\n",
    "from mhqa.search import make_search_tool\n",
    "\n",
    "\n",
    "def format_paragraph(paragraph):\n",
    "    text = paragraph[\"paragraph_text\"]\n",
    "    title = paragraph[\"title\"]\n",
    "    return f\"# {title}\\n{text}\"\n",
    "\n",
    "\n",
    "def make_example(record):\n",
    "    docs = [{\"text\": format_paragraph(p), \"idx\": p[\"idx\"]} for p in record[\"paragraphs\"]]\n",
    "    return dspy.Example(\n",
    "        id=record[\"id\"],\n",
    "        question=record[\"question\"],\n",
    "        docs=docs,\n",
    "        question_decomposition=record[\"question_decomposition\"],\n",
    "        answers=[record[\"answer\"], *record[\"answer_aliases\"]],\n",
    "    ).with_inputs(\"question\", \"docs\")\n",
    "\n",
    "\n",
    "def make_program():\n",
    "    search_tool = make_search_tool()\n",
    "    return ReAct(\"question -> answer\", tools=[search_tool])\n",
    "\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    scores = compute_scores(pred.answer, example.answers)\n",
    "    return scores[\"f1\"]\n",
    "\n",
    "\n",
    "def dynamic_import(module, name):\n",
    "    import importlib\n",
    "\n",
    "    return getattr(importlib.import_module(module), name)\n",
    "\n",
    "\n",
    "def make_optimizer(optimizer_config: dict):\n",
    "    cls = dynamic_import(\"dspy.teleprompt\", optimizer_config[\"class\"])\n",
    "    kwargs = deepcopy(optimizer_config[\"params\"])\n",
    "    if optimizer_config[\"with_metric\"]:\n",
    "        kwargs[\"metric\"] = evaluate_answer\n",
    "    return cls(**kwargs)\n",
    "\n",
    "\n",
    "def preprocess_result(result):\n",
    "    example, pred, score = result\n",
    "    predictions = {f\"predicted_{k}\": v for k, v in dict(pred).items()}\n",
    "    return {**dict(example), **predictions, \"score\": float(score)}\n",
    "\n",
    "\n",
    "def make_results_dataframe(results):\n",
    "    dataf = pd.json_normalize([preprocess_result(result) for result in results])\n",
    "    dataf[\"n_hops\"] = dataf[\"question_decomposition\"].apply(len)\n",
    "    dataf[\"predicted_answer\"] = dataf[\"predicted_answer\"].fillna(\"No Answer\")\n",
    "    return compute_scores_dataframe(dataf)\n",
    "\n",
    "\n",
    "def train_main(\n",
    "    dataset_path: str = typer.Option(..., help=\"Path to the dataset\"),\n",
    "    dataset_name: str = typer.Option(..., help=\"Name of the dataset\"),\n",
    "    dataset_split: str = typer.Option(..., help=\"Dataset split to use (e.g., 'train', 'validation')\"),\n",
    "    model: str = typer.Option(..., help=\"Name of the model to use\"),\n",
    "    temperature: float = typer.Option(..., help=\"Temperature parameter for the model\"),\n",
    "    load_from: str = typer.Option(default=\"UNSET\", help=\"Path to a saved model to load\"),\n",
    "    optimizer_path: Path = typer.Option(..., help=\"Path to the optimizer config\"),\n",
    "    ensemble: str = typer.Option(\"no\", help=\"Whether to use an ensemble of models\"),\n",
    "    out: Path = typer.Option(..., help=\"Output file for trained program\"),\n",
    "):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up LLM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [make_example(record) for record in ds]\n",
    "    print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "    # Create the program\n",
    "    program = make_program()\n",
    "    if load_from and load_from != \"UNSET\":\n",
    "        print(f\"Loading model from {load_from}\")\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Train the program\n",
    "    with open(optimizer_path) as f:\n",
    "        optimizer_config = json.load(f)\n",
    "\n",
    "    if optimizer_config:\n",
    "        optimizer = make_optimizer(optimizer_config)\n",
    "        compile_params = optimizer_config.get(\"compile_params\", {})\n",
    "        trained_program = optimizer.compile(program, trainset=examples, **compile_params)\n",
    "    else:\n",
    "        trained_program = program\n",
    "\n",
    "    if ensemble == \"yes\":\n",
    "        ensemble_optimizer = Ensemble(reduce_fn=dspy.majority)\n",
    "        candidate_programs = [x[-1] for x in trained_program.candidate_programs]\n",
    "        trained_program = ensemble_optimizer.compile(candidate_programs)\n",
    "\n",
    "    # Save the trained program\n",
    "    trained_program.save(out)\n",
    "\n",
    "    return trained_program\n",
    "\n",
    "def evaluate_main(\n",
    "    dataset_path: str = typer.Option(..., help=\"Path to the dataset\"),\n",
    "    dataset_name: str = typer.Option(..., help=\"Name of the dataset\"),\n",
    "    dataset_split: str = typer.Option(..., help=\"Dataset split to use (e.g., 'train', 'validation')\"),\n",
    "    model: str = typer.Option(..., help=\"Name of the model to use\"),\n",
    "    temperature: float = typer.Option(..., help=\"Temperature parameter for the model\"),\n",
    "    load_from: str = typer.Option(default=\"UNSET\", help=\"Path to a saved model to load\"),\n",
    "    out: Path = typer.Option(..., help=\"Output directory for generated results\"),\n",
    "):\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up LLM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [make_example(record) for record in ds]\n",
    "    print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "    # Create the program\n",
    "    program = make_program()\n",
    "    if load_from and load_from != \"UNSET\":\n",
    "        print(f\"Loading model from {load_from}\")\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Evaluate the program\n",
    "    evaluate_program = Evaluate(\n",
    "        metric=evaluate_answer,\n",
    "        devset=examples,\n",
    "        num_threads=4,\n",
    "        display_progress=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    _, results = evaluate_program(program)\n",
    "\n",
    "    # Save the results\n",
    "    result_df = make_results_dataframe(results)\n",
    "    result_df.to_json(out / \"results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # Save the scores\n",
    "    scores = aggregate_scores(result_df)\n",
    "    for n_hops in result_df[\"n_hops\"].unique():\n",
    "        scores[f\"{n_hops}hops\"] = aggregate_scores(result_df[result_df[\"n_hops\"] == n_hops])\n",
    "\n",
    "    with open(out / \"scores.json\", \"w\") as f:\n",
    "        json.dump(scores, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model='llama-3.3-70b-tgi'\n",
    "# model='meta-llama/Llama-3.3-70B-Instruct-Turbo'\n",
    "# model='llama3.1:8b-instruct-q8_0'\n",
    "# model='llama-3.1-8b-instant'\n",
    "# model='gemini-2.0-flash-lite-preview-02-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "trained_program_filepath = Path('trained-program.json')\n",
    "\n",
    "trained_program = train_main(\n",
    "    dataset_path='bdsaglam/musique-mini',\n",
    "    dataset_name='answerable',\n",
    "    dataset_split='train[:50]',\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    load_from='UNSET',\n",
    "    optimizer_path='../data/raw/optimizer-configs/bfsrs-light.json',\n",
    "    out=trained_program_filepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mhqa' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mhqa ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "out = Path('out')\n",
    "\n",
    "evaluate_main(\n",
    "    dataset_path='bdsaglam/musique',\n",
    "    dataset_name='answerable',\n",
    "    dataset_split='train[100:200]',\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    load_from=trained_program_filepath,\n",
    "    out=out,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
