{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barispeppy/bds/pipeline-mhqa-dspy/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "import typer\n",
    "from bellem.musique.eval import (\n",
    "    aggregate_scores,\n",
    "    compute_scores,\n",
    "    compute_scores_dataframe,\n",
    ")\n",
    "from bellem.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt.ensemble import Ensemble\n",
    "from rich.console import Console\n",
    "\n",
    "print = Console(stderr=True).print\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_lm(model, temperature):\n",
    "    lm = dspy.LM(\n",
    "        \"openai/\" + model,\n",
    "        temperature=temperature,\n",
    "        cache=False,\n",
    "        api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5Ranker model unicamp-dl/mt5-base-mmarco-v2 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model unicamp-dl/mt5-base-mmarco-v2, this might take a while...\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/barispeppy/bds/pipeline-mhqa-dspy/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 true token set to ▁yes\n",
      "T5 false token set to ▁no\n",
      "Returning normalised scores...\n",
      "Inputs template set to Query: {query} Document: {text} Relevant:\n"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(model_type=\"t5\", model_name=\"unicamp-dl/mt5-base-mmarco-v2\")\n",
    "\n",
    "\n",
    "def retrieve(docs: list[dict], query: str, top_k: int = 3) -> list[dict]:\n",
    "    \"\"\"Reranker retriever implementation.\n",
    "\n",
    "    Args:\n",
    "        docs: List of documents to search in. Each document should be a dict with\n",
    "                'idx' and 'text' fields.\n",
    "        query: Query string to search for\n",
    "        top_k: Number of documents to retrieve (default: 3)\n",
    "\n",
    "    Returns:\n",
    "        List of documents sorted by relevance score\n",
    "    \"\"\"\n",
    "    # Extract text and ids from docs\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    ranking = ranker.rank(query=query, docs=texts, doc_ids=list(range(len(texts))))\n",
    "    return [docs[result.doc_id] for result in ranking.results[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhqa.react import ReAct, RunContext\n",
    "\n",
    "\n",
    "def format_paragraph(paragraph):\n",
    "    text = paragraph[\"paragraph_text\"]\n",
    "    title = paragraph[\"title\"]\n",
    "    return f\"# {title}\\n{text}\"\n",
    "\n",
    "\n",
    "def make_example(record):\n",
    "    docs = [{\"text\": format_paragraph(p), \"idx\": p[\"idx\"]} for p in record[\"paragraphs\"]]\n",
    "    return dspy.Example(\n",
    "        id=record[\"id\"],\n",
    "        question=record[\"question\"],\n",
    "        question_decomposition=record[\"question_decomposition\"],\n",
    "        docs=docs,\n",
    "        answer=record[\"answer\"],\n",
    "        answers=[record[\"answer\"], *record[\"answer_aliases\"]],\n",
    "    ).with_inputs(\"question\", \"context\")\n",
    "\n",
    "\n",
    "def search(query: str, run_context: RunContext) -> list[str]:\n",
    "    \"\"\"Find\"\"\"\n",
    "    retrieved_docs = retrieve(run_context.input[\"docs\"], query, 3)\n",
    "    return [x[\"text\"] for x in retrieved_docs]\n",
    "\n",
    "\n",
    "def make_program():\n",
    "    return ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)])\n",
    "\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    scores = compute_scores(pred.answer, example.answers)\n",
    "    return scores[\"f1\"]\n",
    "\n",
    "\n",
    "def dynamic_import(module, name):\n",
    "    import importlib\n",
    "\n",
    "    return getattr(importlib.import_module(module), name)\n",
    "\n",
    "\n",
    "def make_optimizer(optimizer_config: dict):\n",
    "    cls = dynamic_import(\"dspy.teleprompt\", optimizer_config[\"class\"])\n",
    "    kwargs = deepcopy(optimizer_config[\"params\"])\n",
    "    if optimizer_config[\"with_metric\"]:\n",
    "        kwargs[\"metric\"] = evaluate_answer\n",
    "    return cls(**kwargs)\n",
    "\n",
    "\n",
    "def preprocess_result(result):\n",
    "    example, pred, score = result\n",
    "    predictions = {f\"predicted_{k}\": v for k, v in dict(pred).items()}\n",
    "    return {**dict(example), **predictions, \"score\": float(score)}\n",
    "\n",
    "\n",
    "def make_results_dataframe(results):\n",
    "    dataf = pd.json_normalize([preprocess_result(result) for result in results])\n",
    "    dataf[\"n_hops\"] = dataf[\"question_decomposition\"].apply(len)\n",
    "    dataf[\"predicted_answer\"] = dataf[\"predicted_answer\"].fillna(\"No Answer\")\n",
    "    return compute_scores_dataframe(dataf)\n",
    "\n",
    "\n",
    "def train_main(\n",
    "    dataset_path: str = typer.Option(..., help=\"Path to the dataset\"),\n",
    "    dataset_name: str = typer.Option(..., help=\"Name of the dataset\"),\n",
    "    dataset_split: str = typer.Option(..., help=\"Dataset split to use (e.g., 'train', 'validation')\"),\n",
    "    model: str = typer.Option(..., help=\"Name of the model to use\"),\n",
    "    temperature: float = typer.Option(..., help=\"Temperature parameter for the model\"),\n",
    "    load_from: str = typer.Option(default=\"UNSET\", help=\"Path to a saved model to load\"),\n",
    "    optimizer_path: Path = typer.Option(..., help=\"Path to the optimizer config\"),\n",
    "    ensemble: str = typer.Option(\"no\", help=\"Whether to use an ensemble of models\"),\n",
    "    out: Path = typer.Option(..., help=\"Output file for trained program\"),\n",
    "):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up LLM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [make_example(record) for record in ds]\n",
    "    print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "    # Create the program\n",
    "    program = make_program()\n",
    "    if load_from and load_from != \"UNSET\":\n",
    "        print(f\"Loading model from {load_from}\")\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Train the program\n",
    "    with open(optimizer_path) as f:\n",
    "        optimizer_config = json.load(f)\n",
    "\n",
    "    if optimizer_config:\n",
    "        optimizer = make_optimizer(optimizer_config)\n",
    "        compile_params = optimizer_config.get(\"compile_params\", {})\n",
    "        trained_program = optimizer.compile(program, trainset=examples, **compile_params)\n",
    "    else:\n",
    "        trained_program = program\n",
    "\n",
    "    if ensemble == \"yes\":\n",
    "        ensemble_optimizer = Ensemble(reduce_fn=dspy.majority)\n",
    "        candidate_programs = [x[-1] for x in trained_program.candidate_programs]\n",
    "        trained_program = ensemble_optimizer.compile(candidate_programs)\n",
    "\n",
    "    # Save the trained program\n",
    "    trained_program.save(out)\n",
    "\n",
    "    return trained_program\n",
    "\n",
    "def evaluate_main(\n",
    "    dataset_path: str = typer.Option(..., help=\"Path to the dataset\"),\n",
    "    dataset_name: str = typer.Option(..., help=\"Name of the dataset\"),\n",
    "    dataset_split: str = typer.Option(..., help=\"Dataset split to use (e.g., 'train', 'validation')\"),\n",
    "    model: str = typer.Option(..., help=\"Name of the model to use\"),\n",
    "    temperature: float = typer.Option(..., help=\"Temperature parameter for the model\"),\n",
    "    load_from: str = typer.Option(default=\"UNSET\", help=\"Path to a saved model to load\"),\n",
    "    out: Path = typer.Option(..., help=\"Output directory for generated results\"),\n",
    "):\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up LLM\n",
    "    configure_lm(model, temperature)\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    ds = load_dataset(dataset_path, dataset_name, split=dataset_split)\n",
    "    examples = [make_example(record) for record in ds]\n",
    "    print(f\"Loaded {len(examples)} examples\")\n",
    "\n",
    "    # Create the program\n",
    "    program = make_program()\n",
    "    if load_from and load_from != \"UNSET\":\n",
    "        print(f\"Loading model from {load_from}\")\n",
    "        program.load(load_from)\n",
    "\n",
    "    # Evaluate the program\n",
    "    evaluate_program = Evaluate(\n",
    "        metric=evaluate_answer,\n",
    "        devset=examples,\n",
    "        num_threads=1,\n",
    "        display_progress=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    _, results = evaluate_program(program)\n",
    "\n",
    "    # Save the results\n",
    "    result_df = make_results_dataframe(results)\n",
    "    result_df.to_json(out / \"results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # Save the scores\n",
    "    scores = aggregate_scores(result_df)\n",
    "    for n_hops in result_df[\"n_hops\"].unique():\n",
    "        scores[f\"{n_hops}hops\"] = aggregate_scores(result_df[result_df[\"n_hops\"] == n_hops])\n",
    "\n",
    "    with open(out / \"scores.json\", \"w\") as f:\n",
    "        json.dump(scores, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m300\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n",
      "Average Metric: 3.80 / 100 (3.8%):  33%|███▎      | 100/300 [32:53<1:19:15, 23.78s/it]"
     ]
    }
   ],
   "source": [
    "out = Path('out')\n",
    "\n",
    "trained_program = train_main(\n",
    "    dataset_path='bdsaglam/musique-mini',\n",
    "    dataset_name='answerable',\n",
    "    dataset_split='train',\n",
    "    # model='llama-3.1-8b-instant',\n",
    "    model='llama3.1:8b-instruct-q8_0',\n",
    "    temperature=0.1,\n",
    "    load_from='UNSET',\n",
    "    optimizer_path='../data/raw/optimizer-configs/bfsrs-medium.json',\n",
    "    out=out,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
